stages:
  generate_activations:
    matrix:
      config:
        - llm: llama_3.1_8b_instruct
          dataset: nq_open
          prompt: qa/short_few_shot_sep
          batch_size: 64
        - llm: llama_3_8b_instruct
          dataset: common_claim
          prompt: cc/binary
          batch_size: 64
        - llm: llama_3_8b_instruct
          dataset: common_claim
          prompt: cc/binary_pretrained
          batch_size: 64
        - llm: llama_3_8b_instruct
          dataset: common_claim
          prompt: cc/binary_cot
          batch_size: 64
        - llm: llama_3_8b
          dataset: common_claim
          prompt: cc/binary
          batch_size: 64
        - llm: llama_3_8b
          dataset: common_claim
          prompt: cc/binary_pretrained
          batch_size: 64
      seed:
        - 42
      generation_config:
        - sampling_low_temp_with_activations
        - sampling_high_temp_with_activations

    cmd: >-
      PYTHONPATH=. python scripts/dataset/generate_activations.py
      dataset=${item.config.dataset}
      llm=${item.config.llm}
      generation_config=${item.generation_config}
      prompt=${item.config.prompt}
      batch_size=${item.config.batch_size}
      results_dir=data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}
      random_seed=${item.seed}
    deps:
      - scripts/dataset/generate_activations.py
      - config/llm/${item.config.llm}.yaml
      - config/generation_config/${item.generation_config}.yaml
      - config/dataset/${item.config.dataset}.yaml
      - config/generate_activations.yaml
      - config/prompt/${item.config.prompt}.yaml
    outs:
      - data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/answers.json
      - data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/config.yaml
      - data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/activations

  eval_answers:
    matrix:
      config:
        - llm: llama_3.1_8b_instruct
          dataset: nq_open
          prompt: qa/short_few_shot_sep
          batch_size: 64
      seed:
        - 42
      generation_config:
        - sampling_low_temp_with_activations
        - sampling_high_temp_with_activations
    cmd: >-
      PYTHONPATH=. python scripts/eval/compute_qa_metrics.py
      --answers-file=data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/answers.json
    deps:
      - scripts/eval/compute_qa_metrics.py
      - data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/answers.json
    outs:
      - data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/metrics.json

  eval_answers_llm_as_judge:
    matrix:
      config:
        - llm: llama_3.1_8b_instruct
          dataset: nq_open
          prompt: qa/short_few_shot_sep
          batch_size: 64
      judge_prompt:
        - llm_as_judge/qa_eval
      seed:
        - 42
      generation_config:
        - sampling_low_temp_with_activations
        - sampling_high_temp_with_activations
    cmd: >-
      PYTHONPATH=. python scripts/eval/llm_as_judge.py
      answers_file=data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/answers.json
      prompt=${item.judge_prompt}
    deps:
      - scripts/eval/llm_as_judge.py
      - data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/answers.json
      - config/llm_as_judge.yaml
      - config/prompt/${item.judge_prompt}.yaml
    outs:
      - data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/llm_judge.json
      - data/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/llm_judge_config.yaml

  generate_trajectories:
    matrix:
      config:
        - llm: llama_3_8b
          dataset: common_claim
          lens: tuned
          prompt: cc/binary_pretrained
        - llm: llama_3_8b_instruct
          dataset: common_claim
          lens: tuned
          prompt: cc/binary
        - llm: llama_3_8b_instruct
          dataset: common_claim
          lens: tuned
          prompt: cc/binary_pretrained
        - llm: llama_3_8b_instruct
          dataset: common_claim
          lens: tuned
          prompt: cc/binary_cot
        - llm: llama_3_8b
          dataset: mmlu
          lens: tuned
          prompt: mmlu/pretrained
        - llm: llama_3_8b_instruct
          dataset: mmlu
          lens: tuned
          prompt: mmlu/default
        - llm: llama_3_8b_instruct
          dataset: mmlu
          lens: tuned
          prompt: mmlu/simple
        - llm: llama_3_8b_instruct
          dataset: mmlu
          lens: tuned
          prompt: mmlu/pretrained
        - llm: llama_3_8b_instruct
          dataset: mmlu
          lens: tuned
          prompt: mmlu/cot
      seed:
        - 42
    cmd: >-
      PYTHONPATH=. python scripts/trajectory/generate_trajectories.py
      dataset=${item.config.dataset}
      llm=${item.config.llm}
      llm.torch_dtype=float32
      lens=${item.config.lens}
      topk_tokens=100
      prompt=${item.config.prompt}
      results_dir=data/trajectories/${item.config.dataset}/${item.config.llm}/${item.config.lens}/${item.config.prompt}/seed_${item.seed}
      random_seed=${item.seed}
    deps:
      - scripts/trajectory/generate_trajectories.py
      - config/llm/${item.config.llm}.yaml
      - config/dataset/${item.config.dataset}.yaml
      - config/prompt/${item.config.prompt}.yaml
      - config/generate_trajectories.yaml
    outs:
      - data/trajectories/${item.config.dataset}/${item.config.llm}/${item.config.lens}/${item.config.prompt}/seed_${item.seed}/answers.jsonl
      - data/trajectories/${item.config.dataset}/${item.config.llm}/${item.config.lens}/${item.config.prompt}/seed_${item.seed}/stats.jsonl
      - data/trajectories/${item.config.dataset}/${item.config.llm}/${item.config.lens}/${item.config.prompt}/seed_${item.seed}/config.yaml

  generate_activations_multiple_samples:
    matrix: &multiple_samples_activation_matrix
      config:
        - llm: llama_3.1_8b_instruct
          dataset: nq_open
          prompt: qa/short_few_shot_sep
          batch_size: 8
      seed:
        - 42
      generation_config:
        - sampling_multiple_samples

    cmd: >-
      PYTHONPATH=. python scripts/dataset/generate_activations.py
      dataset=${item.config.dataset}
      llm=${item.config.llm}
      generation_config=${item.generation_config}
      prompt=${item.config.prompt}
      multiple_samples=true
      batch_size=${item.config.batch_size}
      results_dir=/data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}
      random_seed=${item.seed}
    deps:
      - scripts/dataset/generate_activations.py
      - config/llm/${item.config.llm}.yaml
      - config/generation_config/${item.generation_config}.yaml
      - config/dataset/${item.config.dataset}.yaml
      - config/generate_activations.yaml
      - config/prompt/${item.config.prompt}.yaml
    outs:
      - /data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/answers.json:
          cache: false
      - /data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/config.yaml:
          cache: false
      - /data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/activations:
          cache: false

  compute_semantic_entropy:
    matrix: *multiple_samples_activation_matrix
    cmd: >-
      PYTHONPATH=. python scripts/eval/compute_semantic_entropy.py
      --results-dir=/data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}
    deps:
      - scripts/eval/compute_semantic_entropy.py
      - /data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/answers.json
      - /data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/activations
    outs:
      - /data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/entropy_metrics.json:
          cache: false

  eval_answers_llm_as_judge_multiple_samples:
    matrix: *multiple_samples_activation_matrix
    cmd: >-
      PYTHONPATH=. python scripts/eval/llm_as_judge.py
      answers_file=/data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/answers.json
      answer_column_name=low_temperature
      prompt=llm_as_judge/qa_eval
    deps:
      - scripts/eval/llm_as_judge.py
      - /data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/answers.json
      - config/llm_as_judge.yaml
      - config/prompt/llm_as_judge/qa_eval.yaml
    outs:
      - /data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/llm_judge.json:
          cache: false
      - /data/hallucinations/activations/${item.config.dataset}/${item.config.llm}/${item.generation_config}__prompt_${item.config.prompt}__seed_${item.seed}/llm_judge_config.yaml:
          cache: false
