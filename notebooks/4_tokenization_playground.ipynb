{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tabulate import tabulate\n",
    "\n",
    "from hallucinations.datasets import get_dataset, prepare_dataset\n",
    "from hallucinations.config import QaDatasetConfig, QaPromptConfig\n",
    "from hallucinations.utils import load_and_resolve_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f2ea8af2fc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'messages'],\n",
       "    num_rows: 3610\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_cfg = load_and_resolve_config(\"config/dataset/nq_open.yaml\")\n",
    "prompt_cfg = load_and_resolve_config(\"config/prompt/qa/short_zero_shot.yaml\")\n",
    "ds_config = QaDatasetConfig(**ds_cfg)\n",
    "prompt_config = QaPromptConfig(**prompt_cfg)\n",
    "\n",
    "dataset = prepare_dataset(dataset_config=ds_config, prompt=prompt_config, split=ds_config.test_split_name, use_output=False)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893afdb5ed6c4c15885b10f382dc1cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model to cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "device = next(model.parameters()).device\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(f\"Loaded model to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([5, 61])\n",
      "===\n",
      "Example input:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Answer the following question as briefly as possible.\n",
      "Question: when was the last time anyone was on the moon\n",
      "Answer:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding system prompt\n",
    "# data = [[{\"role\": \"system\", \"content\": \"You're helpful assistant\"}] + x for x in next(dataset.iter(batch_size=5))[\"messages\"]]\n",
    "data = next(dataset.iter(batch_size=5))[\"messages\"]\n",
    "\n",
    "chat_inputs = tokenizer.apply_chat_template(\n",
    "    data,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "encoded_inputs = tokenizer(\n",
    "    chat_inputs,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"longest\",\n",
    "    truncation=False,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "print(f\"shape: {encoded_inputs['input_ids'].shape}\")\n",
    "print(\"===\")\n",
    "print(f\"Example input:\\n{chat_inputs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = {k: v.to(device) for k, v in encoded_inputs.items()}\n",
    "\n",
    "outs = model.generate(\n",
    "    **encoded_inputs,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=1,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    return_dict_in_generate=True,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "type(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Keys: {outs.keys()}\")\n",
    "print(f\"Input shape: {encoded_inputs['input_ids'].shape}\")\n",
    "print(f\"Output shape: {outs.sequences.shape}\")\n",
    "print(f\"Generated shape: {outs.sequences[:, encoded_inputs['input_ids'].size(1):].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_only_special_tokens_mask(token_ids: list[int]) -> list[int]:\n",
    "    if isinstance(token_ids, torch.Tensor):\n",
    "        token_ids = token_ids.tolist()\n",
    "    special_token_ids = set(tokenizer.added_tokens_decoder) | set(tokenizer.all_special_ids)\n",
    "    return [int(tok_id in special_token_ids) for tok_id in token_ids]\n",
    "\n",
    "seq = outs.sequences[0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(seq)\n",
    "mask = get_decoder_only_special_tokens_mask(seq)\n",
    "# mask = tokenizer.get_special_tokens_mask(seq, already_has_special_tokens=True)\n",
    "list(zip(tokens, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden states shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#dim_0 (#new_tokens)\", len(outs[\"hidden_states\"]))\n",
    "print(\"#dim_1 (#layers)\", len(outs[\"hidden_states\"][0]))\n",
    "print(\"+\".center(20, \"+\"))\n",
    "print()\n",
    "\n",
    "shapes = []\n",
    "decoded_tokens = [tokenizer.convert_ids_to_tokens(tok) for tok in outs.sequences]\n",
    "\n",
    "for i_gen_tok, gen_tok_data in enumerate(outs[\"hidden_states\"]):\n",
    "    for i_layer, layer_gen_tok_data in enumerate(gen_tok_data):\n",
    "        tokens = tokenizer.convert_tokens_to_string([decoded_tokens[i][i_gen_tok+(encoded_inputs['input_ids'].size(1)-1)] for i in range(len(decoded_tokens))])\n",
    "        shapes.append([i_gen_tok, i_layer, layer_gen_tok_data.shape, tokens])\n",
    "    shapes.append([\"-\", \"-\", \"-\", \"-\"])\n",
    "\n",
    "print(tabulate(shapes, headers=[\"gen_tok\", \"layer\", \"shape\", \"token\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hallucinations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
