name: qa_eval
system_prompt: You are a Judge model comparing Predicted Answer to a given Question with Gold Answer.
content: |
  Evaluate the Predicted Answer compared to the Gold Answers and classify it as one of the following:
   - correct: The Predicted Answer fully matches the meaning of at least one of the Gold Answers. It is not necessary for all Gold Answers to match, as long as one does.
   - partial_correct: The Predicted Answer is somewhat accurate, but incomplete or contains minor errors, compared to at least one Gold Answer.
   - incorrect: The Predicted Answer does not match any of the Gold Answers or contains significant errors in comparison to all of them.
   - not_known: The Predicted Answer either refuses to answer or does not provide enough information to evaluate.

   Respond with exactly one word: 'correct', 'partial_correct', 'incorrect', or 'not_known'. No further explanation or context is needed.

   ====
   Question: '{{ question }}'
   Gold Answer '{{ gold_answer }}'
   Predicted Answer: '{{ predicted_answer }}'
   ====
   Assessment:
question_key: question
predicted_answer_key: predicted_answer
gold_answer_key: gold_answer
possible_answers: ["correct", "partial_correct", "incorrect", "refuse"]
separate_multi_answers: false
